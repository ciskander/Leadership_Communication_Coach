"""
transcript_parser.py — Multi-format transcript parser.

Supported: .vtt, .srt, .txt, .docx, .pdf
"""
from __future__ import annotations

import io
import logging
import re
import unicodedata
from pathlib import Path
from typing import Optional

from .config import TRANSCRIPT_MAX_WORDS, TRANSCRIPT_MIN_CHARS
from .models import ParsedTranscript, TranscriptMetadata, Turn

logger = logging.getLogger(__name__)

# Boilerplate patterns to strip
_BOILERPLATE_RE = re.compile(
    r"(Transcribed by Otter\.ai|DISCLAIMER:|This transcript was produced|"
    r"Auto-generated by|Powered by Rev\.com|www\.[a-z]+\.com)",
    re.IGNORECASE,
)

_HTML_TAG_RE = re.compile(r"<[^>]+>")
_VOICE_TAG_RE = re.compile(r"<v ([^>]+)>")  # <v Speaker Name>
_TIMESTAMP_INLINE_RE = re.compile(r"\b\d{1,2}:\d{2}(:\d{2})?(\.\d+)?\b")

# VTT header / special block markers
_VTT_BLOCK_SKIP_RE = re.compile(r"^(STYLE|NOTE|REGION)\b", re.IGNORECASE)

# SRT index lines (digit-only lines)
_SRT_INDEX_RE = re.compile(r"^\d+$")
# SRT/VTT timecode
_TIMECODE_RE = re.compile(
    r"\d{2}:\d{2}:\d{2}[.,]\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}[.,]\d{3}"
)


# ── Exceptions ────────────────────────────────────────────────────────────────

class TranscriptParseError(ValueError):
    pass


# ── Utilities ─────────────────────────────────────────────────────────────────

def _normalize_text(text: str) -> str:
    """Strip BOM, normalize line endings, normalize unicode."""
    text = text.lstrip("\ufeff")
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    # Normalize unicode to NFC
    text = unicodedata.normalize("NFC", text)
    return text


def _strip_html(text: str) -> str:
    return _HTML_TAG_RE.sub("", text).strip()


def _clean_speaker(label: str) -> str:
    """Trim and title-case speaker label."""
    return label.strip()


def _dedupe_speakers(speakers: list[str]) -> list[str]:
    """Case-insensitive deduplicate preserving first-seen casing."""
    seen: dict[str, str] = {}
    for s in speakers:
        key = s.lower()
        if key not in seen:
            seen[key] = s
    return list(seen.values())


def _normalize_speaker_map(turns: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """
    Normalise speaker labels across all turns (case-insensitive dedup).
    Returns turns with unified speaker labels.
    """
    canon: dict[str, str] = {}
    for speaker, _ in turns:
        key = speaker.lower()
        if key not in canon:
            canon[key] = speaker
    return [(canon[speaker.lower()], text) for speaker, text in turns]


def _merge_consecutive(turns: list[tuple[str, str]]) -> list[tuple[str, str]]:
    """Merge consecutive turns from the same speaker."""
    if not turns:
        return []
    merged: list[tuple[str, str]] = [turns[0]]
    for speaker, text in turns[1:]:
        if speaker.lower() == merged[-1][0].lower():
            merged[-1] = (merged[-1][0], merged[-1][1] + " " + text)
        else:
            merged.append((speaker, text))
    return merged


def _to_turn_objects(raw: list[tuple[str, str]]) -> list[Turn]:
    return [
        Turn(turn_id=i + 1, speaker_label=speaker, text=text.strip())
        for i, (speaker, text) in enumerate(raw)
        if text.strip()
    ]


def _word_count(turns: list[Turn]) -> int:
    return sum(len(t.text.split()) for t in turns)


def _truncate_at_word_boundary(turns: list[Turn], max_words: int) -> list[Turn]:
    """Drop whole turns once cumulative word count exceeds max_words."""
    result = []
    total = 0
    for turn in turns:
        wc = len(turn.text.split())
        if total + wc > max_words:
            break
        result.append(turn)
        total += wc
    return result


# ── VTT Parser ────────────────────────────────────────────────────────────────

def _parse_vtt(text: str) -> list[Turn]:
    """
    Parse WebVTT content.
    Handles: voice tags <v Speaker>, "Speaker: text", no-attribution.
    """
    lines = text.splitlines()
    raw_turns: list[tuple[str, str]] = []

    # Skip WEBVTT header line
    start = 0
    for i, line in enumerate(lines):
        if line.strip().upper() == "WEBVTT":
            start = i + 1
            break

    i = start
    while i < len(lines):
        line = lines[i].strip()

        # Skip STYLE / NOTE / REGION blocks
        if _VTT_BLOCK_SKIP_RE.match(line):
            while i < len(lines) and lines[i].strip():
                i += 1
            i += 1
            continue

        # Timecode line
        if _TIMECODE_RE.search(line):
            i += 1
            # Collect cue payload lines
            payload_lines: list[str] = []
            while i < len(lines) and lines[i].strip():
                payload_lines.append(lines[i])
                i += 1
            if payload_lines:
                _process_vtt_cue(payload_lines, raw_turns)
            continue

        i += 1

    raw_turns = _normalize_speaker_map(raw_turns)
    raw_turns = _merge_consecutive(raw_turns)
    return _to_turn_objects(raw_turns)


def _process_vtt_cue(payload_lines: list[str], raw_turns: list[tuple[str, str]]) -> None:
    for line in payload_lines:
        # Style A: <v Speaker Name>text
        voice_match = _VOICE_TAG_RE.match(line)
        if voice_match:
            speaker = _clean_speaker(voice_match.group(1))
            text = _strip_html(line[voice_match.end():])
            if text:
                raw_turns.append((speaker, text))
            continue

        # Strip remaining HTML tags
        clean = _strip_html(line)
        if not clean:
            continue

        # Style B: "Speaker: text"
        if ": " in clean:
            parts = clean.split(": ", 1)
            speaker = _clean_speaker(parts[0])
            text = parts[1].strip()
            if speaker and text:
                raw_turns.append((speaker, text))
                continue

        # Style C: no attribution
        raw_turns.append(("Unknown", clean))


# ── SRT Parser ────────────────────────────────────────────────────────────────

def _parse_srt(text: str) -> list[Turn]:
    """Parse SRT subtitle format."""
    lines = text.splitlines()
    raw_turns: list[tuple[str, str]] = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        # Index line
        if _SRT_INDEX_RE.match(line):
            i += 1
            # Timecode
            if i < len(lines) and _TIMECODE_RE.search(lines[i]):
                i += 1
            payload_lines = []
            while i < len(lines) and lines[i].strip():
                payload_lines.append(lines[i].strip())
                i += 1
            for pl in payload_lines:
                clean = _strip_html(pl)
                if not clean:
                    continue
                if ": " in clean:
                    parts = clean.split(": ", 1)
                    raw_turns.append((_clean_speaker(parts[0]), parts[1].strip()))
                else:
                    raw_turns.append(("Unknown", clean))
            continue
        i += 1

    raw_turns = _normalize_speaker_map(raw_turns)
    raw_turns = _merge_consecutive(raw_turns)
    return _to_turn_objects(raw_turns)


# ── TXT Parser ────────────────────────────────────────────────────────────────

def _match_rate(lines: list[str], pattern: re.Pattern) -> float:
    """Fraction of non-empty lines that match pattern."""
    non_empty = [l for l in lines if l.strip()]
    if not non_empty:
        return 0.0
    return sum(1 for l in non_empty if pattern.search(l)) / len(non_empty)


# Sub-format patterns
_FMT_A_RE = re.compile(r"^([A-Za-z][^:]{0,60}):\s+\S")  # "Speaker: text"
_FMT_B_SPEAKER_TS_RE = re.compile(r"^([A-Za-z][^:]{0,60})\s{2,}\d{1,2}:\d{2}")  # "Speaker  HH:MM"
_FMT_E_RE = re.compile(r"^([A-Za-z][^:]{0,60})\s*\(\d{1,2}:\d{2}")  # "Speaker (HH:MM):"


def _parse_txt(text: str) -> list[Turn]:
    lines = text.splitlines()

    # Try Format A: "Speaker: text"
    if _match_rate(lines, _FMT_A_RE) > 0.5:
        return _parse_txt_format_a(lines)

    # Try Format E: "Speaker (timestamp):\ntext"
    if _match_rate(lines, _FMT_E_RE) > 0.3:
        return _parse_txt_format_e(lines)

    # Try Format B: "Speaker  timestamp\ntext"
    if _match_rate(lines, _FMT_B_SPEAKER_TS_RE) > 0.3:
        return _parse_txt_format_b(lines)

    # Try Format D: "Speaker\ntext" separated by blank lines
    turns_d = _parse_txt_format_d(lines)
    if len(turns_d) > 1:
        return turns_d

    # Fallback: single turn
    full = " ".join(l for l in lines if l.strip())
    return [Turn(turn_id=1, speaker_label="Unknown", text=full)]


def _parse_txt_format_a(lines: list[str]) -> list[Turn]:
    raw: list[tuple[str, str]] = []
    current_speaker: Optional[str] = None
    current_text: list[str] = []

    for line in lines:
        line = line.strip()
        if not line or _BOILERPLATE_RE.search(line):
            continue
        m = _FMT_A_RE.match(line)
        if m:
            if current_speaker and current_text:
                raw.append((current_speaker, " ".join(current_text)))
            current_speaker = _clean_speaker(m.group(1))
            rest = line[m.end():].strip()
            current_text = [rest] if rest else []
        else:
            if current_speaker:
                current_text.append(line)
            # else drop orphaned lines

    if current_speaker and current_text:
        raw.append((current_speaker, " ".join(current_text)))

    raw = _normalize_speaker_map(raw)
    raw = _merge_consecutive(raw)
    return _to_turn_objects(raw)


def _parse_txt_format_b(lines: list[str]) -> list[Turn]:
    """Otter.ai: 'Speaker  HH:MM\ntext' blocks."""
    raw: list[tuple[str, str]] = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if _BOILERPLATE_RE.search(line):
            i += 1
            continue
        m = _FMT_B_SPEAKER_TS_RE.match(line)
        if m:
            speaker = _clean_speaker(m.group(1))
            i += 1
            text_lines = []
            while i < len(lines) and lines[i].strip() and not _FMT_B_SPEAKER_TS_RE.match(lines[i]):
                text_lines.append(lines[i].strip())
                i += 1
            if text_lines:
                raw.append((speaker, " ".join(text_lines)))
        else:
            i += 1
    raw = _normalize_speaker_map(raw)
    raw = _merge_consecutive(raw)
    return _to_turn_objects(raw)


def _parse_txt_format_d(lines: list[str]) -> list[Turn]:
    """'Speaker\ntext\n\n' blocks separated by blank lines."""
    raw: list[tuple[str, str]] = []
    blocks: list[list[str]] = []
    current_block: list[str] = []
    for line in lines:
        if line.strip():
            current_block.append(line.strip())
        else:
            if current_block:
                blocks.append(current_block)
                current_block = []
    if current_block:
        blocks.append(current_block)

    for block in blocks:
        if len(block) >= 2:
            speaker = _clean_speaker(block[0])
            text = " ".join(block[1:])
            # Heuristic: first line should look like a name (no colon, < 50 chars)
            if len(speaker) < 50 and ":" not in speaker:
                raw.append((speaker, text))
    if len(raw) < 2:
        return []
    raw = _normalize_speaker_map(raw)
    raw = _merge_consecutive(raw)
    return _to_turn_objects(raw)


def _parse_txt_format_e(lines: list[str]) -> list[Turn]:
    """'Speaker (HH:MM):\ntext' blocks."""
    raw: list[tuple[str, str]] = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        m = _FMT_E_RE.match(line)
        if m:
            speaker = _clean_speaker(m.group(1))
            i += 1
            text_lines = []
            while i < len(lines) and lines[i].strip() and not _FMT_E_RE.match(lines[i]):
                text_lines.append(lines[i].strip())
                i += 1
            if text_lines:
                raw.append((speaker, " ".join(text_lines)))
        else:
            i += 1
    raw = _normalize_speaker_map(raw)
    raw = _merge_consecutive(raw)
    return _to_turn_objects(raw)


# ── DOCX / PDF extraction ─────────────────────────────────────────────────────

def _extract_docx_text(data: bytes) -> str:
    import docx  # python-docx
    doc = docx.Document(io.BytesIO(data))
    return "\n".join(p.text for p in doc.paragraphs)


def _extract_pdf_text(data: bytes) -> str:
    try:
        import fitz  # PyMuPDF
        with fitz.open(stream=data, filetype="pdf") as pdf_doc:
            return "\n".join(page.get_text() for page in pdf_doc)
    except ImportError:
        pass
    import pdfplumber
    with pdfplumber.open(io.BytesIO(data)) as pdf:
        return "\n".join(p.extract_text() or "" for p in pdf.pages)


# ── Public API ────────────────────────────────────────────────────────────────

def parse_transcript(
    data: bytes,
    filename: str,
    source_id: str,
) -> ParsedTranscript:
    """
    Parse a transcript file.

    Args:
        data: Raw file bytes.
        filename: Original filename (used to detect format).
        source_id: Meeting ID or baseline-pack ID (e.g. M-000042).

    Returns:
        ParsedTranscript

    Raises:
        TranscriptParseError: On unsupported format or too-small input.
    """
    if len(data) < TRANSCRIPT_MIN_CHARS:
        raise TranscriptParseError(f"Upload too small ({len(data)} bytes).")

    suffix = Path(filename).suffix.lower()

    # Normalize to text where possible
    raw_text: Optional[str] = None
    detected_format: str

    if suffix == ".docx":
        raw_text = _extract_docx_text(data)
        detected_format = "docx"
    elif suffix == ".pdf":
        raw_text = _extract_pdf_text(data)
        detected_format = "pdf"
    else:
        try:
            raw_text = data.decode("utf-8")
        except UnicodeDecodeError:
            raw_text = data.decode("latin-1", errors="replace")
        detected_format = suffix.lstrip(".")

    raw_text = _normalize_text(raw_text or "")
    # Strip boilerplate lines globally
    raw_text = "\n".join(
        line for line in raw_text.splitlines() if not _BOILERPLATE_RE.search(line)
    )

    if len(raw_text.strip()) < TRANSCRIPT_MIN_CHARS:
        raise TranscriptParseError("Transcript text too short after extraction.")

    # ── Format detection ──
    first_non_blank = next((l.strip() for l in raw_text.splitlines() if l.strip()), "")

    if suffix == ".vtt" or first_non_blank.upper() == "WEBVTT":
        turns = _parse_vtt(raw_text)
        detected_format = "vtt"
    elif suffix == ".srt" or (
        _SRT_INDEX_RE.match(first_non_blank) and _TIMECODE_RE.search(raw_text)
    ):
        turns = _parse_srt(raw_text)
        detected_format = "srt"
    elif detected_format in ("docx", "pdf"):
        turns = _parse_txt(raw_text)
    else:
        turns = _parse_txt(raw_text)
        detected_format = "txt"

    if not turns:
        turns = [Turn(turn_id=1, speaker_label="Unknown", text=raw_text.strip()[:5000])]

    # ── Word count / truncation ──
    wc = _word_count(turns)
    truncated = False
    if wc > TRANSCRIPT_MAX_WORDS:
        logger.warning(
            "Transcript %s has %d words; truncating to %d.", source_id, wc, TRANSCRIPT_MAX_WORDS
        )
        turns = _truncate_at_word_boundary(turns, TRANSCRIPT_MAX_WORDS)
        wc = _word_count(turns)
        truncated = True
        # Renumber turn_ids after truncation
        turns = [Turn(turn_id=i + 1, **t.model_dump(exclude={"turn_id"})) for i, t in enumerate(turns)]

    speaker_labels = _dedupe_speakers([t.speaker_label for t in turns])

    return ParsedTranscript(
        source_id=source_id,
        turns=turns,
        speaker_labels=speaker_labels,
        metadata=TranscriptMetadata(
            original_format=detected_format,
            turn_count=len(turns),
            word_count=wc,
            truncated=truncated,
        ),
    )
